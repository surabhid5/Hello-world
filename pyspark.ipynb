{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soumil Nitin Shah \n",
    "* Bachelor in Electronic Engineering |\n",
    "* Masters in Electrical Engineering | \n",
    "* Master in Computer Engineering |\n",
    "\n",
    "* Website : https://soumilshah.herokuapp.com\n",
    "* Github: https://github.com/soumilshah1995\n",
    "* Linkedin: https://www.linkedin.com/in/shah-soumil/\n",
    "* Blog: https://soumilshah1995.blogspot.com/\n",
    "* Youtube : https://www.youtube.com/channel/UC_eOodxvwS_H7x2uLQa-svw?view_as=subscriber\n",
    "* Facebook Page : https://www.facebook.com/soumilshah1995/\n",
    "* Email : shahsoumil519@gmail.com\n",
    "* projects : https://soumilshah.herokuapp.com/project\n",
    "\n",
    "\n",
    "Hello! I’m Soumil Nitin Shah, a Software and Hardware Developer based in New York City. I have completed by Bachelor in Electronic Engineering and my Double master’s in Computer and Electrical Engineering. I Develop Python Based Cross Platform Desktop Application , Webpages , Software, REST API, Database and much more I have more than 2 Years of Experience in Python\n",
    "\n",
    "\n",
    "## Authors\n",
    "\n",
    "* **Soumil Nitin Shah** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Pyspark in a easy way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\python38\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\python38\\lib\\site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "# Installing pyspark \n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JTSTDSSHAH2.jobtarget.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyProcess</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x143270e6b20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"MyProcess\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: understadning how to create a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"city\", \"type\", \"price\"]\n",
    "data   = map(lambda r: (r[0], r[1], float(r[2])),\n",
    "  map(lambda x: x.split(\",\"),\n",
    "    [\"Paris,Food,19.00\", \"Marseille,Clothing,12.00\",\n",
    "     \"Paris,Food,8.00\", \"Paris,Clothing,15.00\",\n",
    "     \"Marseille,Food,20.00\", \"Lyon,Book,10.00\"]))\n",
    "df = spark.createDataFrame(data, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     city|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris|Clothing| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paris', 'Food', 19.0),\n",
       " ('Marseille', 'Clothing', 12.0),\n",
       " ('Paris', 'Food', 8.0),\n",
       " ('Paris', 'Clothing', 15.0),\n",
       " ('Marseille', 'Food', 20.0),\n",
       " ('Lyon', 'Book', 10.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data   = map(lambda r: (r[0], r[1], float(r[2])),\n",
    "  map(lambda x: x.split(\",\"),\n",
    "    [\"Paris,Food,19.00\", \"Marseille,Clothing,12.00\",\n",
    "     \"Paris,Food,8.00\", \"Paris,Clothing,15.00\",\n",
    "     \"Marseille,Food,20.00\", \"Lyon,Book,10.00\"]))\n",
    "list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comments \n",
    "* One thing we understood is we can create a dataframe by defining a tuple inside a list this is a standard format \n",
    "* we also know that tuple is immutable that means we cannot change value inside tuple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are several methods to retrieve / display the contents of a dataframe:\n",
    "* .collect() method that retrieves the dataframe to the user as a python list.\n",
    "* .take(n) method that retrieves the « n » first elements of the dataframe.\n",
    "* .show() method that displays the contents on the standard output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic information about  Schema and hwo we can manipulate it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Paris', type='Food', price=19.0),\n",
       " Row(city='Marseille', type='Clothing', price=12.0),\n",
       " Row(city='Paris', type='Food', price=8.0),\n",
       " Row(city='Paris', type='Clothing', price=15.0),\n",
       " Row(city='Marseille', type='Food', price=20.0),\n",
       " Row(city='Lyon', type='Book', price=10.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Paris', type='Food', price=19.0),\n",
       " Row(city='Marseille', type='Clothing', price=12.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('city', 'string'), ('type', 'string'), ('price', 'double')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     city|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris|Clothing| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "data = map(lambda r: (r[0], r[1], float(r[2])),\n",
    "  map(lambda x: x.split(\",\"),\n",
    "    [\"Paris,Food,19.00\", \"Marseille,Clothing,12.00\",\n",
    "     \"Paris,Food,8.00\", \"Paris,Clothing,15.00\",\n",
    "     \"Marseille,Food,20.00\", \"Lyon,Book,10.00\"]))\n",
    "\n",
    "schema = StructType([\n",
    "\tStructField(\"city\",  StringType(), nullable=True),\n",
    "\tStructField(\"type\",  StringType(), nullable=True),\n",
    "\tStructField(\"price\", FloatType(),  nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion \n",
    "* we learned how to create a datagframe and how we can also specify the datatypes of dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting Columns in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     city|\n",
      "+---------+\n",
      "|    Paris|\n",
      "|Marseille|\n",
      "|    Paris|\n",
      "|    Paris|\n",
      "|Marseille|\n",
      "|     Lyon|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|     city|    type|\n",
      "+---------+--------+\n",
      "|    Paris|    Food|\n",
      "|Marseille|Clothing|\n",
      "+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting Multiple Columns \n",
    "df.select([\"city\", \"type\"]).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+\n",
      "| city|    type|price|\n",
      "+-----+--------+-----+\n",
      "|Paris|    Food| 19.0|\n",
      "|Paris|    Food|  8.0|\n",
      "|Paris|Clothing| 15.0|\n",
      "+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.city == \"Paris\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| city|type|price|\n",
      "+-----+----+-----+\n",
      "|Paris|Food| 19.0|\n",
      "|Paris|Food|  8.0|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.city == \"Paris\").filter(df.type == \"Food\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| city|type|price|\n",
      "+-----+----+-----+\n",
      "|Paris|Food| 19.0|\n",
      "|Paris|Food|  8.0|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\n",
    "    (df.city == \"Paris\") & (df.type == \"Food\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| city|\n",
      "+-----+\n",
      "|Paris|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\n",
    "    (df.city == \"Paris\") & (df.price > 18.0) ).select('city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|  8.0|\n",
      "| 10.0|\n",
      "| 12.0|\n",
      "| 15.0|\n",
      "| 19.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.price < 20 ).orderBy(df.price.asc()).select(\"price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|  8.0|\n",
      "| 10.0|\n",
      "| 12.0|\n",
      "| 15.0|\n",
      "| 19.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.price < 20 ).sort(df.price.asc()).select(\"price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+---+\n",
      "|     city|    type|price|six|\n",
      "+---------+--------+-----+---+\n",
      "|    Paris|    Food| 19.0|  6|\n",
      "|Marseille|Clothing| 12.0|  6|\n",
      "|    Paris|    Food|  8.0|  6|\n",
      "|    Paris|Clothing| 15.0|  6|\n",
      "|Marseille|    Food| 20.0|  6|\n",
      "|     Lyon|    Book| 10.0|  6|\n",
      "+---------+--------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, rand\n",
    "df = df.withColumn(\"six\", lit(6)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+---------+\n",
      "|     city|    type|price|50percent|\n",
      "+---------+--------+-----+---------+\n",
      "|    Paris|    Food| 19.0|      9.5|\n",
      "|Marseille|Clothing| 12.0|      6.0|\n",
      "|    Paris|    Food|  8.0|      4.0|\n",
      "|    Paris|Clothing| 15.0|      7.5|\n",
      "|Marseille|    Food| 20.0|     10.0|\n",
      "|     Lyon|    Book| 10.0|      5.0|\n",
      "+---------+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, rand\n",
    "\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "\n",
    "data = map(lambda r: (r[0], r[1], float(r[2])),\n",
    "  map(lambda x: x.split(\",\"),\n",
    "    [\"Paris,Food,19.00\", \"Marseille,Clothing,12.00\",\n",
    "     \"Paris,Food,8.00\", \"Paris,Clothing,15.00\",\n",
    "     \"Marseille,Food,20.00\", \"Lyon,Book,10.00\"]))\n",
    "\n",
    "schema = StructType([\n",
    "\tStructField(\"city\",  StringType(), nullable=True),\n",
    "\tStructField(\"type\",  StringType(), nullable=True),\n",
    "\tStructField(\"price\", FloatType(),  nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df = df.withColumn(\"50percent\", df.price / 2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     city|price|\n",
      "+---------+-----+\n",
      "|    Paris| 19.0|\n",
      "|Marseille| 12.0|\n",
      "|    Paris|  8.0|\n",
      "|    Paris| 15.0|\n",
      "|Marseille| 20.0|\n",
      "|     Lyon| 10.0|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(df.type).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|type|\n",
      "+----+\n",
      "|Food|\n",
      "|Food|\n",
      "|Food|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(*[\"city\", \"price\"]).filter(df.type == \"Food\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     CITY|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris|Clothing| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"city\", \"CITY\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advanced Manipulations\n",
    "* In order to respond to most situations, PySpark already has a number of pre-implemented functions present in the pyspark.sql.functions module.\n",
    "\n",
    "* The functions are numerous and of all types. Do not hesitate to refer to it.\n",
    "* Mathematical functions (cos, sin, tan, abs, exp, log…).\n",
    "* Dataframes manipulation (concatenate, repartitioning, structure manipulation...).\n",
    "* Statistics (average, variance, correlation, covariance…).\n",
    "* Date manipulation.\n",
    "* Operators of relational algebra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "| x1| x2|            cos_sum|\n",
      "+---+---+-------------------+\n",
      "|  1|  2|-0.9899924966004454|\n",
      "|  2|  3|0.28366218546322625|\n",
      "|  3|  4| 0.7539022543433046|\n",
      "|  4|  5|-0.9111302618846769|\n",
      "+---+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cos\n",
    "df = spark.createDataFrame(\n",
    "  [[1, 2], [2, 3], [3, 4], [4, 5]],\n",
    "  schema=[\"x1\", \"x2\"])\n",
    "df.withColumn(\"cos_sum\", cos(df.x1 + df.x2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Group by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|State Count|\n",
      "+-----------+\n",
      "|          4|\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").count().withColumnRenamed(\"count\", \"State Count\").select(\"State Count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+----------+\n",
      "|state|avg(salary)|avg(age)|avg(bonus)|\n",
      "+-----+-----------+--------+----------+\n",
      "|   CA|    87500.0|   29.75|   22000.0|\n",
      "|   NY|    85800.0|    45.8|   17000.0|\n",
      "+-----+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UDF user defined function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "@udf(\"int\")  # decorator to define an UDF to return an int\n",
    "def add_3(x):\n",
    "    return x * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|SALARY|\n",
      "+------+\n",
      "|270000|\n",
      "|258000|\n",
      "|243000|\n",
      "|270000|\n",
      "|297000|\n",
      "|249000|\n",
      "|237000|\n",
      "|240000|\n",
      "|273000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(add_3(df.salary).alias('SALARY')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|SALARY|\n",
      "+------+\n",
      "|270000|\n",
      "|258000|\n",
      "|243000|\n",
      "|270000|\n",
      "|297000|\n",
      "|249000|\n",
      "|237000|\n",
      "|240000|\n",
      "|273000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(add_3(df.salary).alias('SALARY')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|SALARY|\n",
      "+------+\n",
      "|270000|\n",
      "|258000|\n",
      "|243000|\n",
      "|270000|\n",
      "|297000|\n",
      "|249000|\n",
      "|237000|\n",
      "|240000|\n",
      "|273000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(add_3(df.salary).alias('SALARY')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas to pysaprk and vice versa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert pyspark to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_name</th>\n",
       "      <th>department</th>\n",
       "      <th>state</th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>bonus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td>Sales</td>\n",
       "      <td>NY</td>\n",
       "      <td>90000</td>\n",
       "      <td>34</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael</td>\n",
       "      <td>Sales</td>\n",
       "      <td>NY</td>\n",
       "      <td>86000</td>\n",
       "      <td>56</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td>Sales</td>\n",
       "      <td>CA</td>\n",
       "      <td>81000</td>\n",
       "      <td>30</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maria</td>\n",
       "      <td>Finance</td>\n",
       "      <td>CA</td>\n",
       "      <td>90000</td>\n",
       "      <td>24</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Raman</td>\n",
       "      <td>Finance</td>\n",
       "      <td>CA</td>\n",
       "      <td>99000</td>\n",
       "      <td>40</td>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Scott</td>\n",
       "      <td>Finance</td>\n",
       "      <td>NY</td>\n",
       "      <td>83000</td>\n",
       "      <td>36</td>\n",
       "      <td>19000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jen</td>\n",
       "      <td>Finance</td>\n",
       "      <td>NY</td>\n",
       "      <td>79000</td>\n",
       "      <td>53</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jeff</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>CA</td>\n",
       "      <td>80000</td>\n",
       "      <td>25</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kumar</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>NY</td>\n",
       "      <td>91000</td>\n",
       "      <td>50</td>\n",
       "      <td>21000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  employee_name department state  salary  age  bonus\n",
       "0         James      Sales    NY   90000   34  10000\n",
       "1       Michael      Sales    NY   86000   56  20000\n",
       "2        Robert      Sales    CA   81000   30  23000\n",
       "3         Maria    Finance    CA   90000   24  23000\n",
       "4         Raman    Finance    CA   99000   40  24000\n",
       "5         Scott    Finance    NY   83000   36  19000\n",
       "6           Jen    Finance    NY   79000   53  15000\n",
       "7          Jeff  Marketing    CA   80000   25  18000\n",
       "8         Kumar  Marketing    NY   91000   50  21000"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark to pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlCtx = SQLContext(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlCtx.createDataFrame(df).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing null rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(spark)\n",
    "\n",
    "df = sqlCtx.createDataFrame([\n",
    "    [1,'Navee','Srikanth']\n",
    "    , [2,'','Srikanth'] ,\n",
    "    [3,'Naveen','']],\n",
    "    ['ID','FirstName','LastName']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+\n",
      "| ID|FirstName|LastName|\n",
      "+---+---------+--------+\n",
      "|  1|    Navee|Srikanth|\n",
      "|  2|         |Srikanth|\n",
      "|  3|   Naveen|        |\n",
      "+---+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is all things you can do to remove null values but for removing custom value we have to apply a function and filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+\n",
      "| ID|FirstName|LastName|\n",
      "+---+---------+--------+\n",
      "|  1|    Navee|Srikanth|\n",
      "|  2|         |Srikanth|\n",
      "|  3|   Naveen|        |\n",
      "+---+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()\n",
    "df.filter(df.FirstName.isNotNull()).show()\n",
    "df.where(df.FirstName.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(spark)\n",
    "\n",
    "df = sqlCtx.createDataFrame([\n",
    "    [1,'Navee','Srikanth']\n",
    "    , [2,'','Srikanth'] ,\n",
    "    [3,'Naveen','']],\n",
    "    ['ID','FirstName','LastName']\n",
    ")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "def remove(x):\n",
    "    if x == '':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# NOTE: it seems that calls to udf() must be after SparkContext() is called\n",
    "udfValueToCategory = udf(remove, IntegerType())\n",
    "\n",
    "df11 = df.withColumn(\"category\", udfValueToCategory(\"FirstName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+--------+\n",
      "| ID|FirstName|LastName|category|\n",
      "+---+---------+--------+--------+\n",
      "|  1|    Navee|Srikanth|       0|\n",
      "|  2|         |Srikanth|       1|\n",
      "|  3|   Naveen|        |       0|\n",
      "+---+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code snippets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| x1| x2|   x3|\n",
      "+---+---+-----+\n",
      "|  1|  a| 23.0|\n",
      "|  3|  B|-23.0|\n",
      "+---+---+-----+\n",
      "\n",
      "+---+---+-----+--------+\n",
      "| x1| x2|   x3|category|\n",
      "+---+---+-----+--------+\n",
      "|  1|  a| 23.0|    cat1|\n",
      "|  3|  B|-23.0|     n/a|\n",
      "+---+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Snippets from Stack overflow https://stackoverflow.com/questions/33681487/how-do-i-add-a-new-column-to-a-spark-dataframe-using-pyspark/33683462\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(spark)\n",
    "df = sqlCtx.createDataFrame(\n",
    "    [(1, \"a\", 23.0), (3, \"B\", -23.0)], (\"x1\", \"x2\", \"x3\"))\n",
    "\n",
    "df.show()\n",
    "def valueToCategory(value):\n",
    "   if   value == 1: return 'cat1'\n",
    "   elif value == 2: return 'cat2'\n",
    "   else: return 'n/a'\n",
    "\n",
    "# NOTE: it seems that calls to udf() must be after SparkContext() is called\n",
    "udfValueToCategory = udf(valueToCategory, StringType())\n",
    "df_with_cat = df.withColumn(\"category\", udfValueToCategory(\"x1\"))\n",
    "df_with_cat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
